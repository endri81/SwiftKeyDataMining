---
title: 'Capstone Coursera Project Report'
subtitle: ' SwiftKey Data Mining'
author: 'Endri Raco'
output:
  pdf_document:
    df_print: kable
    toc: yes
documentclass: report
classoption: a4paper
fig_height: 5
fig_width: 5
fontsize: 10pt
highlight: zenburn
latex_engine: xelatex
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
urlcolor: blue
---

---



```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## specify the packages needed
if(!require(vroom)) install.packages('vroom', 
repos = 'http://cran.us.r-project.org')
if(!require(stringr)) install.packages('stringr', 
repos = 'http://cran.us.r-project.org')
if(!require(tidyverse)) install.packages('tidyverse', 
repos = 'http://cran.us.r-project.org')
if(!require(tm)) install.packages('tm', 
repos = 'http://cran.us.r-project.org')
if(!require(tidytext)) install.packages('tidytext', 
repos = 'http://cran.us.r-project.org')
if(!require(textclean)) install.packages('textclean', 
repos = 'http://cran.us.r-project.org')
## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = '90%', fig.align = 'center',
                      tidy.opts=list(width.cutoff=60),
                      tidy=TRUE,
                      cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

## Dedication

This project and all my work is dedicated to victims of Covid19 in Albania.

&nbsp;

## Acknowledgement

I would like to express my special thanks of gratitude to Prof. Jeff Leek, Prof. Roger Peng
and Prof. Brian Caffo for the wonderful material and thorough explanations they provided during all courses. Also I want to thank my friends of this course who share the same interests for Data Science. 

&nbsp;

## Introduction

Around the world, people are spending an increasing amount of time on their mobile devices for email, social networking, banking and a whole range of other activities. But typing on mobile devices can be a serious pain. [SwiftKey](https://www.microsoft.com/en-us/swiftkey), built a smart keyboard that makes it easier for people to type on their mobile devices. 

One cornerstone of their smart keyboard is predictive text models When user types some text,  the keyboard presents three options for what the next word might be. 

The aim of this project is to build  a predictive model of  text and illustrate its functionality in a Shiny App.

&nbsp;

For all project calculations is used the following PC:

```{r pc}
print('Operating System:')
version
```

&nbsp;

## Methods and Analysis
### Importing data

&nbsp;

Let's start by downloading data in our project inside **data** folder.

&nbsp;

```{r, data_download, eval=FALSE}
# Set seed for reproducible results
set.seed(12345)
# Create variable that saves url : data_url
data_url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
# Check if directory data exist in working directory
if (!file.exists("./data")) {
    dir.create("./data") # if not create
}
# Check if file exists inside directory data
if (!file.exists("./data/Coursera-SwiftKey.zip")) {
    download.file(data_url, destfile = "./data/Coursera-SwiftKey.zip", mode = "wb") # if not download
    Download_Date <- Sys.time() # save download date
}
# Check if extracted folder exists
if (!file.exists("./data/final")) {
    unzip(zipfile = "./data/Coursera-SwiftKey.zip", exdir = "./data") # if not unzip 
}

pathFiles <- file.path("./data/final", "en_US")
files <- list.files(pathFiles, recursive = TRUE)
files
```
&nbsp;

Let's check the result of **unzip** 

```{r, unzip-files}
list.files("./data/",recursive = TRUE)
```

We see that as result of unzipping ***Coursera-SwiftKey.zip** we now have folder **final** with four other folders inside (de_DE, en_US, fi_FI, ru_RU)

Each folder contains 3 text files named by this rule :

  - **File1** : language_pattern.twitter

  - **File2** : language_pattern.blogs

  - **File3** : language_pattern.news


## Loading the data in

  - We are not going to load the entire dataset in to build your algorithms. For now we will read only files in English. Let's identify our selected files by using language pattern:
  
```{r, english-data}
# Identify existing files with english pattern
list.files("./data/", pattern = "^en_US", recursive = TRUE)
```
  
Now let's read the data using **readLines** function:


```{r, load-data}
# Read data in separate files
# Read blogs
blogs <- vroom_lines("./data/final/en_US/en_US.blogs.txt")
# Read news
news <- vroom_lines("./data/final/en_US/en_US.news.txt")
# Read twitter
twitter <- vroom_lines("./data/final/en_US/en_US.twitter.txt")
```


## Sampling Data

For the model we will build, we will not need all of the data. We will use **rbinom** function to take a random sample of size 5% of each file.

```{r, sample-data}
# Sample data
# Set seed for reproducible results
set.seed(12345)
# Function to random sample out of given files : binom_sample
binom_sample <- function(orig_f) {
# Get length of original files
file_length <- length(orig_f)
# Use rbinom to sample
orig_f[rbinom(file_length *0.05,length(orig_f),0.5)]
}
# Use binom_sample to sample blogs: sample_blog
sample_blog <- binom_sample(blogs)
# Use binom_sample to sample news: sample_news
sample_news <- binom_sample(news)
# Use binom_sample to sample twitter: sample_twitter
sample_twitter <- binom_sample(twitter)
# Remove unneeded objects
rm(blogs, news, twitter, binom_sample)
```


## Data Tidying

We will get our files in **tidy text** format to make process of cleaning more straightforward. 
To get the tidy format we will get use of **tidytext** package. 

We will use **unnest_tokens** function to tokenize the text, meaning to split the text in single words (unigrams) and later on in groups of two and three words(bigrams and trigrams). 

With these words we will create so called **bag of words** meaning we dont care for now about structure or grammar.


```{r, tidy-data}
# Covert sample blog : tidy_blog
tidy_blog <- sample_blog %>%
  data_frame(text = .) %>%
  unnest_tokens(word, text, format = "text")
# Covert sample news : tidy_news
tidy_news <- sample_news %>%
  data_frame(text = .) %>%
  unnest_tokens(word, text, format = "text")
# Covert sample twitter: tidy_twitter
tidy_twitter <- sample_twitter %>%
  data_frame(text = .) %>%
  unnest_tokens(word, text, format = "text")
# Remove unneeded objects
rm(sample_blog, sample_news, sample_twitter)
```


## Data Cleaning

Now we will proceed data cleaning  using tidyverse grammar. We will create a function that will do all the cleaning for us

```{r, stop_num}
# Read a list of swearwords
swear <- read_csv('./data/swearWords.csv')
# Create unary function
stop_num <- . %>% 
  # Remove stopwords
  anti_join(get_stopwords()) %>% 
  # Remove numbers
  filter(is.na(as.numeric(word))) %>% 
  # Remove everything not a-z and -
  filter(grepl("[^a-z'-.]", word) == FALSE) %>%
  # Only one space left
  filter(grepl("[\\s]+", word) == FALSE) %>%
  # Remove twitter hashtags
  filter(grepl("#\\S+",  word) == FALSE) %>%
  # Remove mentions
  filter(grepl("@\\S+",  word) == FALSE) %>%
  # Remove special characters
  filter(grepl("[[:cntrl:]]",  word) == FALSE) %>%
  # Remove HTML/XML
  filter(grepl("<[^>]*>",  word) == FALSE) %>%
  # Remove URL
  filter(grepl("http\\S+",  word) == FALSE) %>%
  # remove everything non-english  
  filter(grepl("[^[:alnum:][:space:]]",  word) == FALSE) %>%
  # remove single letter or two
  filter(nchar(word) > 2) %>%
  # Remove swearwords
  anti_join(swear, by = 'word') %>%
  # Remove punctation and spaces
  mutate(word = str_replace_all(word,"[:punct:]|[:space:]","")) %>%
  # remove letters repeated more than 2 times
  mutate(word = str_replace_all(word, "(.)\\1+", "\\1"))
```

Let's clean:

```{r, clean-data}
# Clean stopwords from tidy_blog
tidy_blog <- tidy_blog %>% stop_num()
# Clean stopwords from tidy_news
tidy_news <- tidy_news %>% stop_num()
# Clean stopwords from tidy_twitter
tidy_twitter <- tidy_twitter %>% stop_num()
# Remove unneeded objects
rm(swear, stop_num)
```


Finally we have our reasonably cleaned data as tibble for making our following step of exploratory data analysis more straightforward.


## Exploratory Data Analysis

After cleaning our data a little bit, a natural step would be getting some insight on our data. Since the main question in text mining is to quantify the meaning of text, one way is to measure frequencies of words. Before 
starting to collect this measurement , we first will split text in groups of one, two and three words (unigrams, bigrams, trigrams). With these words we will create so called **bag of words** meaning we dont care for now about structure or grammar.



```{r, unigram}
# Unigrams
tidy_counts <- clean_tbl %>%
# Tokenize by word
unnest_tokens(word, text)%>%
# Count by word
count(word) 
# View
head(tidy_counts, 20)
```

Now lets check grafically the above result

```{r, top_20_freq}
# Visualise
ggplot(head(tidy_counts, 20), aes(x = word,y = n)) +
geom_bar(stat='identity',colour="white", fill =fillColor) +
geom_text(aes(x = word, y = 1, label = paste0("(",n,")",sep="")),
hjust=0, vjust=.5, size = 4, colour = 'black',
fontface = 'bold') +
labs(x = 'Word', y = 'Word Count', 
title = 'Top 20 Most Frequent Words') +
coord_flip() + 
  theme_bw()
```





